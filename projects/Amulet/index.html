<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content=".">
    <meta name="keywords" content="LLM Test Time Alignment, Tuning-free Alignment">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/icon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LKCDBW8851"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-LKCDBW8851');
</script>


<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>

    </div>
</nav>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs
                    </h1>
                    <!-- <h2 class="title publication-title is-4 has-text-grey"> ICCV 2023 (Oral, Best Paper Finalist)</h2> -->
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wbhu.github.io">Wenbo Hu</a><sup>1</sup>,</span>
                        <span class="author-block">
              <a href="https://wbhu.github.io/projects/Tri-MipRF">Yuling Wang</a><sup>2</sup>,</span>
                        <span class="author-block">
              <a href="https://wbhu.github.io/projects/Tri-MipRF">Lin Ma</a><sup>1</sup>,
            </span>
                        <span class="author-block">
              <a href="https://ybbbbt.com">Bangbang Yang</a><sup>1</sup>,
            </span>
                        <span class="author-block">
              <a href="http://geometrylearning.com/lin">Lin Gao</a><sup>3</sup>,
            </span>
                        <span class="author-block">
              <a href="https://wbhu.github.io/projects/Tri-MipRF">Xiao Liu</a><sup>1</sup>,
            </span>
                        <span class="author-block">
              <a href="https://wbhu.github.io/projects/Tri-MipRF">Yuewen Ma</a><sup>1</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>PICO, ByteDance, Beijing</span>
                        <span class="author-block"><sup>2</sup>Tsinghua University</span>
                        <span class="author-block"><sup>3</sup>Institute of Computing Technology, CAS</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href="https://openreview.net/pdf?id=f9w89OY2cp"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2307.11335"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!--                            &lt;!&ndash; Video Link. &ndash;&gt;-->
                            <!--                            <span class="link-block">-->
                            <!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="fab fa-youtube"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Video</span>-->
                            <!--                </a>-->
                            <!--              </span>-->
                            <!-- Code Link. -->
                            <span class="link-block">
                <a href="https://github.com/zowiezhang/Amulet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
                            <!--                            &lt;!&ndash; Dataset Link. &ndash;&gt;-->
                            <!--                            <span class="link-block">-->
                            <!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="far fa-images"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Data</span>-->
                            <!--                  </a>-->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>



<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Despite the tremendous progress in neural radiance fields (NeRF), we still face a dilemma of the
                        trade-off between quality and efficiency, <i>e.g.</i>, MipNeRF presents fine-detailed and
                        anti-aliased renderings but takes days for training, while Instant-ngp can accomplish the
                        reconstruction in a few minutes but suffers from blurring or aliasing when rendering at various
                        distances or resolutions due to ignoring the sampling area.
                    </p>
                    <p>
                        To this end, we propose a novel Tri-Mip encoding (à la “mipmap”) that enables both instant
                        reconstruction and anti-aliased high-fidelity rendering for neural radiance fields. The key is
                        to factorize the pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can
                        efficiently perform 3D area sampling by taking advantage of 2D pre-filtered feature maps, which
                        significantly elevates the rendering quality without sacrificing efficiency. To cope with the
                        novel Tri-Mip representation, we propose a cone-casting rendering technique to efficiently
                        sample anti-aliased 3D features with the Tri-Mip encoding considering both pixel imaging and
                        observing distance.
                    </p>
                    <p>
                        Extensive experiments on both synthetic and real-world datasets demonstrate our method achieves
                        state-of-the-art rendering quality and reconstruction speed while maintaining a compact
                        representation that reduces 25% model size compared against Instant-ngp.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

        <!--         Paper video.-->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Video</h2>
                <div class="publication-video">
                    <iframe src="https://www.youtube.com/embed/eBgoul4F148"
                            title="Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields"
                            frameborder="0"
                            allow="autoplay; encrypted-media; gyroscope; picture-in-picture; web-share"
                            allowfullscreen></iframe>
                </div>
            </div>
        </div>
        <!--        / Paper video.-->
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">

        <!-- Method. -->
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">Method</h2>

                <!-- Overview. -->
                <!--                <h3 class="title is-4">Overview</h3>-->
                <div class="content has-text-justified">
                    <div class="container is-max-desktop has-text-centered">
                        <img src="img/overview.jpg" width="100%">
                    </div>
                    <p>
                        To render a pixel, we emit a <b>cone</b> from the camera’s projection center to the pixel on the
                        image plane, and then we cast a set of spheres inside the cone. Next, the spheres are
                        orthogonally projected
                        on the three planes and featurized by our <b>Tri-Mip encoding</b>. After that the feature
                        vector is fed into the tiny MLP to non-linearly map to
                        density and color. Finally, the density and
                        color of the spheres are integrated using volume rendering to produce final color for the pixel.
                    </p>
                </div>
                <!--/ Overview. -->

                <h2 class="title is-3">Results</h2>

                <h3 class="title is-4">Quality <i>vs.</i> Reconstruction time</h3>
                <div class="container is-max-desktop has-text-centered">
                    <img src="img/teaser.jpg" width="50%">
                </div>
                <p>
                    Our Tri-MipRF achieves state-of-the-art rendering quality while can be reconstructed efficiently,
                    compared with cutting-edge radiance fields methods, <i>e.g.,</i> NeRF, MipNeRF, Plenoxels,
                    TensoRF, and Instant-ngp. Equipping Instant-ngp with super-sampling (named Instant-ngp<sup>↑5×</sup>)
                    improves the rendering quality to a certain extent but significantly slows down the reconstruction.
                </p>
                <br>

                <h3 class="title is-4">In-the-wild Captures</h3>
                <div class="container is-max-desktop hasxt-centered">
                    <img src="img/in_the_wild.jpg" width="100%">
                </div>
                <p>
                    To further demonstrate the applicability, we captured several objects in the wild, performed SFM
                    on the sequence to estimate the camera’s intrinsic and extrinsic parameters, and applied our
                    Tri-MipRF to reconstruct them. We show three example results here, where we can see the rendered
                    novel views faithfully reproduce the
                    detailed structures and appearances, and the PSNR/SSIM
                    values also evidence the applicability of our method.
                </p>
                <br>


                <!--                <h3 class="title is-4">Hybrid Volume-surface Rendering</h3>-->
                <!--                <div class="container is-max-desktop hasxt-centered">-->
                <!--                    <img src="img/in_the_wild.jpg" width="100%">-->
                <!--                </div>-->
                <!--                <p>-->
                <!--                    To further demonstrate the applicability, we captured several objects in the wild, performed SFM-->
                <!--                    on the sequence to estimate the camera’s intrinsic and extrinsic parameters, and applied our-->
                <!--                    Tri-MipRF to reconstruct them. We show three example results here, where we can see the rendered-->
                <!--                    novel views faithfully reproduce the-->
                <!--                    detailed structures and appearances, and the PSNR/SSIM-->
                <!--                    values also evidence the applicability of our method.-->
                <!--                </p>-->


            </div>
        </div>


        <!-- Concurrent Work. -->
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">Related Links</h2>

                <div class="content has-text-justified">

                    <p>
                        <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>
                        introduces a <b>multi-sampling-based</b> method to address the same problem, efficient
                        anti-aliasing, while our method belongs to the <b>pre-filtering-based</b> method.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Concurrent Work. -->

    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{zhang2025amulet,
    title={Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of {LLM}s},
    author={Zhaowei Zhang and Fengshuo Bai and Qizhi Chen and Chengdong Ma and Mingzhi Wang and Haoran Sun and Zilong Zheng and Yaodong Yang},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=f9w89OY2cp}
}</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Website template credit to <a
                            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
